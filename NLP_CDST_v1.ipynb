{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNmg1Y+IKG8fy9TNx58U14h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beverm2391/NLP-CDST/blob/main/NLP_CDST_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "_vnICff1ZePC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a Tesla T4 GPU to the runtime to support the text encoding process"
      ],
      "metadata": {
        "id": "vn7o7mN7Z4Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scmigID4A4he",
        "outputId": "850df43e-6e93-462d-9841-493b89c0367c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "ojQrIS6WaIpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "6k71JuH6V2ZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327a6d35-4761-4cc5-9198-96fd1d1238f7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.24.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL5TcwO_Bn6k",
        "outputId": "0eaa0323-f3fe-4de7-b782-8bf229c9cc25"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.7/dist-packages (0.25.0)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.7/dist-packages (from openai) (1.2.0.62)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from openai) (4.1.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8fBZDVpjD_c",
        "outputId": "ae68f3e1-7208-42b9-e9eb-73b84fea8d8a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (0.21.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import GPT2TokenizerFast\n",
        "import openai\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "v8qaz5q-JHgF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount google drive to colab so that you can access its files. This is used to save/load relevant data like encoded text."
      ],
      "metadata": {
        "id": "c91GLu0MaW11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y_xs9X8-SWJ",
        "outputId": "82c08c0d-e3ad-4790-b2eb-933fc1dd2676"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I have already taken my context, in this case the DSM-5 and loaded the text of each page into a list. I used a pdf parser for python called pdfminer.six\n",
        "# [\"Page one text\", \"Page two text\",...]\n",
        "\n",
        "# I'm using pickle to save/load the list as binary\n",
        "\n",
        "with open(\"/content/drive/MyDrive/PDFs-for-Parsing/DSM-5_page_list_v1\", \"rb\") as fp:\n",
        "  dsm_list = pickle.load(fp)\n",
        "\n",
        "# I'll refer to this context as text_to_encode"
      ],
      "metadata": {
        "id": "rfEdwQSA-IID"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path to store and load the encoded text (which is in the form of vector embeddings)\n",
        "fpath = \"/content/drive/MyDrive/PDFs-for-Parsing/DSM-5_embeddings_v1\""
      ],
      "metadata": {
        "id": "d7-W98cBc1jk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the OpenAI API key environment variable\n",
        "# load from master.env file in My Drive\n",
        "# this gives access to the GPT- mode\n",
        "# your own API key can be obtained here: https://openai.com/api/\n",
        "\n",
        "load_dotenv(\"/content/drive/MyDrive/master.env\")\n",
        "api_key = os.environ.get('OPENAI-API-KEY')\n",
        "openai.api_key = api_key"
      ],
      "metadata": {
        "id": "sxiKqKEjB6Rr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Some Function Definition"
      ],
      "metadata": {
        "id": "62SIk7kb-yMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function either loads the embeddings, or returns None if none exist."
      ],
      "metadata": {
        "id": "nFciHWHNfpMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(fpath : str) -> List[List[float]]:\n",
        "  # if the csv file containg our context embeddings at fpath exists, read it and convert it into list format\n",
        "  if os.path.exists(fpath):\n",
        "    with open(fpath, 'r') as f:\n",
        "      embeddings_df = pd.read_csv(fpath)\n",
        "      # convert the df back into a list of lists\n",
        "      embeddings = embeddings_df.values.tolist()\n",
        "      print(\"loaded embeddings\")\n",
        "      return embeddings\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "dqGRpmMVmvf9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next funciton encodes text into a vector embedding"
      ],
      "metadata": {
        "id": "kYsxsfrqfvPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
        "\n",
        "This model \"maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search.\n",
        "It has been trained on 215M (question, answer) pairs from diverse sources.\"\n",
        "It will encode our corpus of text into vector embeddings"
      ],
      "metadata": {
        "id": "58Okdn0zcmTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text: str) -> List[float]:\n",
        "  model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
        "  embedding = model.encode(text)\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "4I5sEpn9Yrs8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This funciton makes the API call to GPT-3 and returns the response."
      ],
      "metadata": {
        "id": "CJPKcuZqfCiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(question, corpus):\n",
        "    prompt = build_prompt(question, corpus)\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-002\",\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "    return response['choices'][0]['text'].strip(\" \\n\")"
      ],
      "metadata": {
        "id": "VwE4nP-66HqV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the training data"
      ],
      "metadata": {
        "id": "udAq-ZcGpn0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function iterates over the training text and encodes each list item into a vector embedding, which is then stored in a DataFrame and written to csv for storage. "
      ],
      "metadata": {
        "id": "7_wCBjMfc7JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(text_to_encode : List[str], fpath : str):\n",
        "\n",
        "  embeddings = load_embeddings(fpath)\n",
        "  # create empty list of n items if no embeddings\n",
        "  if embeddings == None:\n",
        "    print(\"No file found, creating blank list\")\n",
        "    embeddings = [None for _ in text_to_encode]\n",
        "\n",
        "  failed_embeddings = []\n",
        "  embeddings_to_generate = embeddings.count(None)\n",
        "\n",
        "  if embeddings_to_generate == 0:\n",
        "    print(\"No embeddings to generate\")\n",
        "    return embeddings\n",
        "\n",
        "  print(f\"Generating {embeddings_to_generate} embeddings\")\n",
        "\n",
        "  for idx, value in enumerate(text_to_encode):\n",
        "    if embeddings[idx] is None:\n",
        "      try:\n",
        "        embedding = get_embedding(value)\n",
        "        embeddings[idx] = embedding.tolist()\n",
        "        print(f\"Embeddding {idx + 1} generated\")\n",
        "      # if embedding fails\n",
        "      except Exception as e:\n",
        "        print(f\"Embedding {idx + 1} failed\")\n",
        "        failed_embeddings.append(idx)\n",
        "\n",
        "  print(\"Encoding Successful\")\n",
        "\n",
        "  if len(failed_embeddings) > 0:\n",
        "    print(f\"{len(failed_embeddings)} embeddings failed. Indexes: {failed_embeddings}\")\n",
        "  else:\n",
        "    print(\"No embeddings failed\")\n",
        "\n",
        "  # This is where we'll write the embeddings to CSV\n",
        "  temp_df_for_storage = pd.DataFrame(embeddings)\n",
        "  temp_df_for_storage.to_csv(fpath, index=False)\n",
        "\n",
        "  print(\"Written to csv\")\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "CtIh-IbLYxv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This encoding process will take some time, even with the T4 GPU. It took me about 15 minutes to encode a list of ~800 pages, each with around 1k tokens."
      ],
      "metadata": {
        "id": "blSq5aVfdIF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = encode_text(dsm_list, fpath)"
      ],
      "metadata": {
        "id": "SCNS9rbNSi8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77fa545-8652-4987-ba88-af9aa48d1256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No file found, creating blank list\n",
            "Generating 806 embeddings\n",
            "Embeddding 1 generated\n",
            "Embeddding 2 generated\n",
            "Embeddding 3 generated\n",
            "Embeddding 4 generated\n",
            "Embeddding 5 generated\n",
            "Embeddding 6 generated\n",
            "Embeddding 7 generated\n",
            "Embeddding 8 generated\n",
            "Embeddding 9 generated\n",
            "Embeddding 10 generated\n",
            "Embeddding 11 generated\n",
            "Embeddding 12 generated\n",
            "Embeddding 13 generated\n",
            "Embeddding 14 generated\n",
            "Embeddding 15 generated\n",
            "Embeddding 16 generated\n",
            "Embeddding 17 generated\n",
            "Embeddding 18 generated\n",
            "Embeddding 19 generated\n",
            "Embeddding 20 generated\n",
            "Embeddding 21 generated\n",
            "Embeddding 22 generated\n",
            "Embeddding 23 generated\n",
            "Embeddding 24 generated\n",
            "Embeddding 25 generated\n",
            "Embeddding 26 generated\n",
            "Embeddding 27 generated\n",
            "Embeddding 28 generated\n",
            "Embeddding 29 generated\n",
            "Embeddding 30 generated\n",
            "Embeddding 31 generated\n",
            "Embeddding 32 generated\n",
            "Embeddding 33 generated\n",
            "Embeddding 34 generated\n",
            "Embeddding 35 generated\n",
            "Embeddding 36 generated\n",
            "Embeddding 37 generated\n",
            "Embeddding 38 generated\n",
            "Embeddding 39 generated\n",
            "Embeddding 40 generated\n",
            "Embeddding 41 generated\n",
            "Embeddding 42 generated\n",
            "Embeddding 43 generated\n",
            "Embeddding 44 generated\n",
            "Embeddding 45 generated\n",
            "Embeddding 46 generated\n",
            "Embeddding 47 generated\n",
            "Embeddding 48 generated\n",
            "Embeddding 49 generated\n",
            "Embeddding 50 generated\n",
            "Embeddding 51 generated\n",
            "Embeddding 52 generated\n",
            "Embeddding 53 generated\n",
            "Embeddding 54 generated\n",
            "Embeddding 55 generated\n",
            "Embeddding 56 generated\n",
            "Embeddding 57 generated\n",
            "Embeddding 58 generated\n",
            "Embeddding 59 generated\n",
            "Embeddding 60 generated\n",
            "Embeddding 61 generated\n",
            "Embeddding 62 generated\n",
            "Embeddding 63 generated\n",
            "Embeddding 64 generated\n",
            "Embeddding 65 generated\n",
            "Embeddding 66 generated\n",
            "Embeddding 67 generated\n",
            "Embeddding 68 generated\n",
            "Embeddding 69 generated\n",
            "Embeddding 70 generated\n",
            "Embeddding 71 generated\n",
            "Embeddding 72 generated\n",
            "Embeddding 73 generated\n",
            "Embeddding 74 generated\n",
            "Embeddding 75 generated\n",
            "Embeddding 76 generated\n",
            "Embeddding 77 generated\n",
            "Embeddding 78 generated\n",
            "Embeddding 79 generated\n",
            "Embeddding 80 generated\n",
            "Embeddding 81 generated\n",
            "Embeddding 82 generated\n",
            "Embeddding 83 generated\n",
            "Embeddding 84 generated\n",
            "Embeddding 85 generated\n",
            "Embeddding 86 generated\n",
            "Embeddding 87 generated\n",
            "Embeddding 88 generated\n",
            "Embeddding 89 generated\n",
            "Embeddding 90 generated\n",
            "Embeddding 91 generated\n",
            "Embeddding 92 generated\n",
            "Embeddding 93 generated\n",
            "Embeddding 94 generated\n",
            "Embeddding 95 generated\n",
            "Embeddding 96 generated\n",
            "Embeddding 97 generated\n",
            "Embeddding 98 generated\n",
            "Embeddding 99 generated\n",
            "Embeddding 100 generated\n",
            "Embeddding 101 generated\n",
            "Embeddding 102 generated\n",
            "Embeddding 103 generated\n",
            "Embeddding 104 generated\n",
            "Embeddding 105 generated\n",
            "Embeddding 106 generated\n",
            "Embeddding 107 generated\n",
            "Embeddding 108 generated\n",
            "Embeddding 109 generated\n",
            "Embeddding 110 generated\n",
            "Embeddding 111 generated\n",
            "Embeddding 112 generated\n",
            "Embeddding 113 generated\n",
            "Embeddding 114 generated\n",
            "Embeddding 115 generated\n",
            "Embeddding 116 generated\n",
            "Embeddding 117 generated\n",
            "Embeddding 118 generated\n",
            "Embeddding 119 generated\n",
            "Embeddding 120 generated\n",
            "Embeddding 121 generated\n",
            "Embeddding 122 generated\n",
            "Embeddding 123 generated\n",
            "Embeddding 124 generated\n",
            "Embeddding 125 generated\n",
            "Embeddding 126 generated\n",
            "Embeddding 127 generated\n",
            "Embeddding 128 generated\n",
            "Embeddding 129 generated\n",
            "Embeddding 130 generated\n",
            "Embeddding 131 generated\n",
            "Embeddding 132 generated\n",
            "Embeddding 133 generated\n",
            "Embeddding 134 generated\n",
            "Embeddding 135 generated\n",
            "Embeddding 136 generated\n",
            "Embeddding 137 generated\n",
            "Embeddding 138 generated\n",
            "Embeddding 139 generated\n",
            "Embeddding 140 generated\n",
            "Embeddding 141 generated\n",
            "Embeddding 142 generated\n",
            "Embeddding 143 generated\n",
            "Embeddding 144 generated\n",
            "Embeddding 145 generated\n",
            "Embeddding 146 generated\n",
            "Embeddding 147 generated\n",
            "Embeddding 148 generated\n",
            "Embeddding 149 generated\n",
            "Embeddding 150 generated\n",
            "Embeddding 151 generated\n",
            "Embeddding 152 generated\n",
            "Embeddding 153 generated\n",
            "Embeddding 154 generated\n",
            "Embeddding 155 generated\n",
            "Embeddding 156 generated\n",
            "Embeddding 157 generated\n",
            "Embeddding 158 generated\n",
            "Embeddding 159 generated\n",
            "Embeddding 160 generated\n",
            "Embeddding 161 generated\n",
            "Embeddding 162 generated\n",
            "Embeddding 163 generated\n",
            "Embeddding 164 generated\n",
            "Embeddding 165 generated\n",
            "Embeddding 166 generated\n",
            "Embeddding 167 generated\n",
            "Embeddding 168 generated\n",
            "Embeddding 169 generated\n",
            "Embeddding 170 generated\n",
            "Embeddding 171 generated\n",
            "Embeddding 172 generated\n",
            "Embeddding 173 generated\n",
            "Embeddding 174 generated\n",
            "Embeddding 175 generated\n",
            "Embeddding 176 generated\n",
            "Embeddding 177 generated\n",
            "Embeddding 178 generated\n",
            "Embeddding 179 generated\n",
            "Embeddding 180 generated\n",
            "Embeddding 181 generated\n",
            "Embeddding 182 generated\n",
            "Embeddding 183 generated\n",
            "Embeddding 184 generated\n",
            "Embeddding 185 generated\n",
            "Embeddding 186 generated\n",
            "Embeddding 187 generated\n",
            "Embeddding 188 generated\n",
            "Embeddding 189 generated\n",
            "Embeddding 190 generated\n",
            "Embeddding 191 generated\n",
            "Embeddding 192 generated\n",
            "Embeddding 193 generated\n",
            "Embeddding 194 generated\n",
            "Embeddding 195 generated\n",
            "Embeddding 196 generated\n",
            "Embeddding 197 generated\n",
            "Embeddding 198 generated\n",
            "Embeddding 199 generated\n",
            "Embeddding 200 generated\n",
            "Embeddding 201 generated\n",
            "Embeddding 202 generated\n",
            "Embeddding 203 generated\n",
            "Embeddding 204 generated\n",
            "Embeddding 205 generated\n",
            "Embeddding 206 generated\n",
            "Embeddding 207 generated\n",
            "Embeddding 208 generated\n",
            "Embeddding 209 generated\n",
            "Embeddding 210 generated\n",
            "Embeddding 211 generated\n",
            "Embeddding 212 generated\n",
            "Embeddding 213 generated\n",
            "Embeddding 214 generated\n",
            "Embeddding 215 generated\n",
            "Embeddding 216 generated\n",
            "Embeddding 217 generated\n",
            "Embeddding 218 generated\n",
            "Embeddding 219 generated\n",
            "Embeddding 220 generated\n",
            "Embeddding 221 generated\n",
            "Embeddding 222 generated\n",
            "Embeddding 223 generated\n",
            "Embeddding 224 generated\n",
            "Embeddding 225 generated\n",
            "Embeddding 226 generated\n",
            "Embeddding 227 generated\n",
            "Embeddding 228 generated\n",
            "Embeddding 229 generated\n",
            "Embeddding 230 generated\n",
            "Embeddding 231 generated\n",
            "Embeddding 232 generated\n",
            "Embeddding 233 generated\n",
            "Embeddding 234 generated\n",
            "Embeddding 235 generated\n",
            "Embeddding 236 generated\n",
            "Embeddding 237 generated\n",
            "Embeddding 238 generated\n",
            "Embeddding 239 generated\n",
            "Embeddding 240 generated\n",
            "Embeddding 241 generated\n",
            "Embeddding 242 generated\n",
            "Embeddding 243 generated\n",
            "Embeddding 244 generated\n",
            "Embeddding 245 generated\n",
            "Embeddding 246 generated\n",
            "Embeddding 247 generated\n",
            "Embeddding 248 generated\n",
            "Embeddding 249 generated\n",
            "Embeddding 250 generated\n",
            "Embeddding 251 generated\n",
            "Embeddding 252 generated\n",
            "Embeddding 253 generated\n",
            "Embeddding 254 generated\n",
            "Embeddding 255 generated\n",
            "Embeddding 256 generated\n",
            "Embeddding 257 generated\n",
            "Embeddding 258 generated\n",
            "Embeddding 259 generated\n",
            "Embeddding 260 generated\n",
            "Embeddding 261 generated\n",
            "Embeddding 262 generated\n",
            "Embeddding 263 generated\n",
            "Embeddding 264 generated\n",
            "Embeddding 265 generated\n",
            "Embeddding 266 generated\n",
            "Embeddding 267 generated\n",
            "Embeddding 268 generated\n",
            "Embeddding 269 generated\n",
            "Embeddding 270 generated\n",
            "Embeddding 271 generated\n",
            "Embeddding 272 generated\n",
            "Embeddding 273 generated\n",
            "Embeddding 274 generated\n",
            "Embeddding 275 generated\n",
            "Embeddding 276 generated\n",
            "Embeddding 277 generated\n",
            "Embeddding 278 generated\n",
            "Embeddding 279 generated\n",
            "Embeddding 280 generated\n",
            "Embeddding 281 generated\n",
            "Embeddding 282 generated\n",
            "Embeddding 283 generated\n",
            "Embeddding 284 generated\n",
            "Embeddding 285 generated\n",
            "Embeddding 286 generated\n",
            "Embeddding 287 generated\n",
            "Embeddding 288 generated\n",
            "Embeddding 289 generated\n",
            "Embeddding 290 generated\n",
            "Embeddding 291 generated\n",
            "Embeddding 292 generated\n",
            "Embeddding 293 generated\n",
            "Embeddding 294 generated\n",
            "Embeddding 295 generated\n",
            "Embeddding 296 generated\n",
            "Embeddding 297 generated\n",
            "Embeddding 298 generated\n",
            "Embeddding 299 generated\n",
            "Embeddding 300 generated\n",
            "Embeddding 301 generated\n",
            "Embeddding 302 generated\n",
            "Embeddding 303 generated\n",
            "Embeddding 304 generated\n",
            "Embeddding 305 generated\n",
            "Embeddding 306 generated\n",
            "Embeddding 307 generated\n",
            "Embeddding 308 generated\n",
            "Embeddding 309 generated\n",
            "Embeddding 310 generated\n",
            "Embeddding 311 generated\n",
            "Embeddding 312 generated\n",
            "Embeddding 313 generated\n",
            "Embeddding 314 generated\n",
            "Embeddding 315 generated\n",
            "Embeddding 316 generated\n",
            "Embeddding 317 generated\n",
            "Embeddding 318 generated\n",
            "Embeddding 319 generated\n",
            "Embeddding 320 generated\n",
            "Embeddding 321 generated\n",
            "Embeddding 322 generated\n",
            "Embeddding 323 generated\n",
            "Embeddding 324 generated\n",
            "Embeddding 325 generated\n",
            "Embeddding 326 generated\n",
            "Embeddding 327 generated\n",
            "Embeddding 328 generated\n",
            "Embeddding 329 generated\n",
            "Embeddding 330 generated\n",
            "Embeddding 331 generated\n",
            "Embeddding 332 generated\n",
            "Embeddding 333 generated\n",
            "Embeddding 334 generated\n",
            "Embeddding 335 generated\n",
            "Embeddding 336 generated\n",
            "Embeddding 337 generated\n",
            "Embeddding 338 generated\n",
            "Embeddding 339 generated\n",
            "Embeddding 340 generated\n",
            "Embeddding 341 generated\n",
            "Embeddding 342 generated\n",
            "Embeddding 343 generated\n",
            "Embeddding 344 generated\n",
            "Embeddding 345 generated\n",
            "Embeddding 346 generated\n",
            "Embeddding 347 generated\n",
            "Embeddding 348 generated\n",
            "Embeddding 349 generated\n",
            "Embeddding 350 generated\n",
            "Embeddding 351 generated\n",
            "Embeddding 352 generated\n",
            "Embeddding 353 generated\n",
            "Embeddding 354 generated\n",
            "Embeddding 355 generated\n",
            "Embeddding 356 generated\n",
            "Embeddding 357 generated\n",
            "Embeddding 358 generated\n",
            "Embeddding 359 generated\n",
            "Embeddding 360 generated\n",
            "Embeddding 361 generated\n",
            "Embeddding 362 generated\n",
            "Embeddding 363 generated\n",
            "Embeddding 364 generated\n",
            "Embeddding 365 generated\n",
            "Embeddding 366 generated\n",
            "Embeddding 367 generated\n",
            "Embeddding 368 generated\n",
            "Embeddding 369 generated\n",
            "Embeddding 370 generated\n",
            "Embeddding 371 generated\n",
            "Embeddding 372 generated\n",
            "Embeddding 373 generated\n",
            "Embeddding 374 generated\n",
            "Embeddding 375 generated\n",
            "Embeddding 376 generated\n",
            "Embeddding 377 generated\n",
            "Embeddding 378 generated\n",
            "Embeddding 379 generated\n",
            "Embeddding 380 generated\n",
            "Embeddding 381 generated\n",
            "Embeddding 382 generated\n",
            "Embeddding 383 generated\n",
            "Embeddding 384 generated\n",
            "Embeddding 385 generated\n",
            "Embeddding 386 generated\n",
            "Embeddding 387 generated\n",
            "Embeddding 388 generated\n",
            "Embeddding 389 generated\n",
            "Embeddding 390 generated\n",
            "Embeddding 391 generated\n",
            "Embeddding 392 generated\n",
            "Embeddding 393 generated\n",
            "Embeddding 394 generated\n",
            "Embeddding 395 generated\n",
            "Embeddding 396 generated\n",
            "Embeddding 397 generated\n",
            "Embeddding 398 generated\n",
            "Embeddding 399 generated\n",
            "Embeddding 400 generated\n",
            "Embeddding 401 generated\n",
            "Embeddding 402 generated\n",
            "Embeddding 403 generated\n",
            "Embeddding 404 generated\n",
            "Embeddding 405 generated\n",
            "Embeddding 406 generated\n",
            "Embeddding 407 generated\n",
            "Embeddding 408 generated\n",
            "Embeddding 409 generated\n",
            "Embeddding 410 generated\n",
            "Embeddding 411 generated\n",
            "Embeddding 412 generated\n",
            "Embeddding 413 generated\n",
            "Embeddding 414 generated\n",
            "Embeddding 415 generated\n",
            "Embeddding 416 generated\n",
            "Embeddding 417 generated\n",
            "Embeddding 418 generated\n",
            "Embeddding 419 generated\n",
            "Embeddding 420 generated\n",
            "Embeddding 421 generated\n",
            "Embeddding 422 generated\n",
            "Embeddding 423 generated\n",
            "Embeddding 424 generated\n",
            "Embeddding 425 generated\n",
            "Embeddding 426 generated\n",
            "Embeddding 427 generated\n",
            "Embeddding 428 generated\n",
            "Embeddding 429 generated\n",
            "Embeddding 430 generated\n",
            "Embeddding 431 generated\n",
            "Embeddding 432 generated\n",
            "Embeddding 433 generated\n",
            "Embeddding 434 generated\n",
            "Embeddding 435 generated\n",
            "Embeddding 436 generated\n",
            "Embeddding 437 generated\n",
            "Embeddding 438 generated\n",
            "Embeddding 439 generated\n",
            "Embeddding 440 generated\n",
            "Embeddding 441 generated\n",
            "Embeddding 442 generated\n",
            "Embeddding 443 generated\n",
            "Embeddding 444 generated\n",
            "Embeddding 445 generated\n",
            "Embeddding 446 generated\n",
            "Embeddding 447 generated\n",
            "Embeddding 448 generated\n",
            "Embeddding 449 generated\n",
            "Embeddding 450 generated\n",
            "Embeddding 451 generated\n",
            "Embeddding 452 generated\n",
            "Embeddding 453 generated\n",
            "Embeddding 454 generated\n",
            "Embeddding 455 generated\n",
            "Embeddding 456 generated\n",
            "Embeddding 457 generated\n",
            "Embeddding 458 generated\n",
            "Embeddding 459 generated\n",
            "Embeddding 460 generated\n",
            "Embeddding 461 generated\n",
            "Embeddding 462 generated\n",
            "Embeddding 463 generated\n",
            "Embeddding 464 generated\n",
            "Embeddding 465 generated\n",
            "Embeddding 466 generated\n",
            "Embeddding 467 generated\n",
            "Embeddding 468 generated\n",
            "Embeddding 469 generated\n",
            "Embeddding 470 generated\n",
            "Embeddding 471 generated\n",
            "Embeddding 472 generated\n",
            "Embeddding 473 generated\n",
            "Embeddding 474 generated\n",
            "Embeddding 475 generated\n",
            "Embeddding 476 generated\n",
            "Embeddding 477 generated\n",
            "Embeddding 478 generated\n",
            "Embeddding 479 generated\n",
            "Embeddding 480 generated\n",
            "Embeddding 481 generated\n",
            "Embeddding 482 generated\n",
            "Embeddding 483 generated\n",
            "Embeddding 484 generated\n",
            "Embeddding 485 generated\n",
            "Embeddding 486 generated\n",
            "Embeddding 487 generated\n",
            "Embeddding 488 generated\n",
            "Embeddding 489 generated\n",
            "Embeddding 490 generated\n",
            "Embeddding 491 generated\n",
            "Embeddding 492 generated\n",
            "Embeddding 493 generated\n",
            "Embeddding 494 generated\n",
            "Embeddding 495 generated\n",
            "Embeddding 496 generated\n",
            "Embeddding 497 generated\n",
            "Embeddding 498 generated\n",
            "Embeddding 499 generated\n",
            "Embeddding 500 generated\n",
            "Embeddding 501 generated\n",
            "Embeddding 502 generated\n",
            "Embeddding 503 generated\n",
            "Embeddding 504 generated\n",
            "Embeddding 505 generated\n",
            "Embeddding 506 generated\n",
            "Embeddding 507 generated\n",
            "Embeddding 508 generated\n",
            "Embeddding 509 generated\n",
            "Embeddding 510 generated\n",
            "Embeddding 511 generated\n",
            "Embeddding 512 generated\n",
            "Embeddding 513 generated\n",
            "Embeddding 514 generated\n",
            "Embeddding 515 generated\n",
            "Embeddding 516 generated\n",
            "Embeddding 517 generated\n",
            "Embeddding 518 generated\n",
            "Embeddding 519 generated\n",
            "Embeddding 520 generated\n",
            "Embeddding 521 generated\n",
            "Embeddding 522 generated\n",
            "Embeddding 523 generated\n",
            "Embeddding 524 generated\n",
            "Embeddding 525 generated\n",
            "Embeddding 526 generated\n",
            "Embeddding 527 generated\n",
            "Embeddding 528 generated\n",
            "Embeddding 529 generated\n",
            "Embeddding 530 generated\n",
            "Embeddding 531 generated\n",
            "Embeddding 532 generated\n",
            "Embeddding 533 generated\n",
            "Embeddding 534 generated\n",
            "Embeddding 535 generated\n",
            "Embeddding 536 generated\n",
            "Embeddding 537 generated\n",
            "Embeddding 538 generated\n",
            "Embeddding 539 generated\n",
            "Embeddding 540 generated\n",
            "Embeddding 541 generated\n",
            "Embeddding 542 generated\n",
            "Embeddding 543 generated\n",
            "Embeddding 544 generated\n",
            "Embeddding 545 generated\n",
            "Embeddding 546 generated\n",
            "Embeddding 547 generated\n",
            "Embeddding 548 generated\n",
            "Embeddding 549 generated\n",
            "Embeddding 550 generated\n",
            "Embeddding 551 generated\n",
            "Embeddding 552 generated\n",
            "Embeddding 553 generated\n",
            "Embeddding 554 generated\n",
            "Embeddding 555 generated\n",
            "Embeddding 556 generated\n",
            "Embeddding 557 generated\n",
            "Embeddding 558 generated\n",
            "Embeddding 559 generated\n",
            "Embeddding 560 generated\n",
            "Embeddding 561 generated\n",
            "Embeddding 562 generated\n",
            "Embeddding 563 generated\n",
            "Embeddding 564 generated\n",
            "Embeddding 565 generated\n",
            "Embeddding 566 generated\n",
            "Embeddding 567 generated\n",
            "Embeddding 568 generated\n",
            "Embeddding 569 generated\n",
            "Embeddding 570 generated\n",
            "Embeddding 571 generated\n",
            "Embeddding 572 generated\n",
            "Embeddding 573 generated\n",
            "Embeddding 574 generated\n",
            "Embeddding 575 generated\n",
            "Embeddding 576 generated\n",
            "Embeddding 577 generated\n",
            "Embeddding 578 generated\n",
            "Embeddding 579 generated\n",
            "Embeddding 580 generated\n",
            "Embeddding 581 generated\n",
            "Embeddding 582 generated\n",
            "Embeddding 583 generated\n",
            "Embeddding 584 generated\n",
            "Embeddding 585 generated\n",
            "Embeddding 586 generated\n",
            "Embeddding 587 generated\n",
            "Embeddding 588 generated\n",
            "Embeddding 589 generated\n",
            "Embeddding 590 generated\n",
            "Embeddding 591 generated\n",
            "Embeddding 592 generated\n",
            "Embeddding 593 generated\n",
            "Embeddding 594 generated\n",
            "Embeddding 595 generated\n",
            "Embeddding 596 generated\n",
            "Embeddding 597 generated\n",
            "Embeddding 598 generated\n",
            "Embeddding 599 generated\n",
            "Embeddding 600 generated\n",
            "Embeddding 601 generated\n",
            "Embeddding 602 generated\n",
            "Embeddding 603 generated\n",
            "Embeddding 604 generated\n",
            "Embeddding 605 generated\n",
            "Embeddding 606 generated\n",
            "Embeddding 607 generated\n",
            "Embeddding 608 generated\n",
            "Embeddding 609 generated\n",
            "Embeddding 610 generated\n",
            "Embeddding 611 generated\n",
            "Embeddding 612 generated\n",
            "Embeddding 613 generated\n",
            "Embeddding 614 generated\n",
            "Embeddding 615 generated\n",
            "Embeddding 616 generated\n",
            "Embeddding 617 generated\n",
            "Embeddding 618 generated\n",
            "Embeddding 619 generated\n",
            "Embeddding 620 generated\n",
            "Embeddding 621 generated\n",
            "Embeddding 622 generated\n",
            "Embeddding 623 generated\n",
            "Embeddding 624 generated\n",
            "Embeddding 625 generated\n",
            "Embeddding 626 generated\n",
            "Embeddding 627 generated\n",
            "Embeddding 628 generated\n",
            "Embeddding 629 generated\n",
            "Embeddding 630 generated\n",
            "Embeddding 631 generated\n",
            "Embeddding 632 generated\n",
            "Embeddding 633 generated\n",
            "Embeddding 634 generated\n",
            "Embeddding 635 generated\n",
            "Embeddding 636 generated\n",
            "Embeddding 637 generated\n",
            "Embeddding 638 generated\n",
            "Embeddding 639 generated\n",
            "Embeddding 640 generated\n",
            "Embeddding 641 generated\n",
            "Embeddding 642 generated\n",
            "Embeddding 643 generated\n",
            "Embeddding 644 generated\n",
            "Embeddding 645 generated\n",
            "Embeddding 646 generated\n",
            "Embeddding 647 generated\n",
            "Embeddding 648 generated\n",
            "Embeddding 649 generated\n",
            "Embeddding 650 generated\n",
            "Embeddding 651 generated\n",
            "Embeddding 652 generated\n",
            "Embeddding 653 generated\n",
            "Embeddding 654 generated\n",
            "Embeddding 655 generated\n",
            "Embeddding 656 generated\n",
            "Embeddding 657 generated\n",
            "Embeddding 658 generated\n",
            "Embeddding 659 generated\n",
            "Embeddding 660 generated\n",
            "Embeddding 661 generated\n",
            "Embeddding 662 generated\n",
            "Embeddding 663 generated\n",
            "Embeddding 664 generated\n",
            "Embeddding 665 generated\n",
            "Embeddding 666 generated\n",
            "Embeddding 667 generated\n",
            "Embeddding 668 generated\n",
            "Embeddding 669 generated\n",
            "Embeddding 670 generated\n",
            "Embeddding 671 generated\n",
            "Embeddding 672 generated\n",
            "Embeddding 673 generated\n",
            "Embeddding 674 generated\n",
            "Embeddding 675 generated\n",
            "Embeddding 676 generated\n",
            "Embeddding 677 generated\n",
            "Embeddding 678 generated\n",
            "Embeddding 679 generated\n",
            "Embeddding 680 generated\n",
            "Embeddding 681 generated\n",
            "Embeddding 682 generated\n",
            "Embeddding 683 generated\n",
            "Embeddding 684 generated\n",
            "Embeddding 685 generated\n",
            "Embeddding 686 generated\n",
            "Embeddding 687 generated\n",
            "Embeddding 688 generated\n",
            "Embeddding 689 generated\n",
            "Embeddding 690 generated\n",
            "Embeddding 691 generated\n",
            "Embeddding 692 generated\n",
            "Embeddding 693 generated\n",
            "Embeddding 694 generated\n",
            "Embeddding 695 generated\n",
            "Embeddding 696 generated\n",
            "Embeddding 697 generated\n",
            "Embeddding 698 generated\n",
            "Embeddding 699 generated\n",
            "Embeddding 700 generated\n",
            "Embeddding 701 generated\n",
            "Embeddding 702 generated\n",
            "Embeddding 703 generated\n",
            "Embeddding 704 generated\n",
            "Embeddding 705 generated\n",
            "Embeddding 706 generated\n",
            "Embeddding 707 generated\n",
            "Embeddding 708 generated\n",
            "Embeddding 709 generated\n",
            "Embeddding 710 generated\n",
            "Embeddding 711 generated\n",
            "Embeddding 712 generated\n",
            "Embeddding 713 generated\n",
            "Embeddding 714 generated\n",
            "Embeddding 715 generated\n",
            "Embeddding 716 generated\n",
            "Embeddding 717 generated\n",
            "Embeddding 718 generated\n",
            "Embeddding 719 generated\n",
            "Embeddding 720 generated\n",
            "Embeddding 721 generated\n",
            "Embeddding 722 generated\n",
            "Embeddding 723 generated\n",
            "Embeddding 724 generated\n",
            "Embeddding 725 generated\n",
            "Embeddding 726 generated\n",
            "Embeddding 727 generated\n",
            "Embeddding 728 generated\n",
            "Embeddding 729 generated\n",
            "Embeddding 730 generated\n",
            "Embeddding 731 generated\n",
            "Embeddding 732 generated\n",
            "Embeddding 733 generated\n",
            "Embeddding 734 generated\n",
            "Embeddding 735 generated\n",
            "Embeddding 736 generated\n",
            "Embeddding 737 generated\n",
            "Embeddding 738 generated\n",
            "Embeddding 739 generated\n",
            "Embeddding 740 generated\n",
            "Embeddding 741 generated\n",
            "Embeddding 742 generated\n",
            "Embeddding 743 generated\n",
            "Embeddding 744 generated\n",
            "Embeddding 745 generated\n",
            "Embeddding 746 generated\n",
            "Embeddding 747 generated\n",
            "Embeddding 748 generated\n",
            "Embeddding 749 generated\n",
            "Embeddding 750 generated\n",
            "Embeddding 751 generated\n",
            "Embeddding 752 generated\n",
            "Embeddding 753 generated\n",
            "Embeddding 754 generated\n",
            "Embeddding 755 generated\n",
            "Embeddding 756 generated\n",
            "Embeddding 757 generated\n",
            "Embeddding 758 generated\n",
            "Embeddding 759 generated\n",
            "Embeddding 760 generated\n",
            "Embeddding 761 generated\n",
            "Embeddding 762 generated\n",
            "Embeddding 763 generated\n",
            "Embeddding 764 generated\n",
            "Embeddding 765 generated\n",
            "Embeddding 766 generated\n",
            "Embeddding 767 generated\n",
            "Embeddding 768 generated\n",
            "Embeddding 769 generated\n",
            "Embeddding 770 generated\n",
            "Embeddding 771 generated\n",
            "Embeddding 772 generated\n",
            "Embeddding 773 generated\n",
            "Embeddding 774 generated\n",
            "Embeddding 775 generated\n",
            "Embeddding 776 generated\n",
            "Embeddding 777 generated\n",
            "Embeddding 778 generated\n",
            "Embeddding 779 generated\n",
            "Embeddding 780 generated\n",
            "Embeddding 781 generated\n",
            "Embeddding 782 generated\n",
            "Embeddding 783 generated\n",
            "Embeddding 784 generated\n",
            "Embeddding 785 generated\n",
            "Embeddding 786 generated\n",
            "Embeddding 787 generated\n",
            "Embeddding 788 generated\n",
            "Embeddding 789 generated\n",
            "Embeddding 790 generated\n",
            "Embeddding 791 generated\n",
            "Embeddding 792 generated\n",
            "Embeddding 793 generated\n",
            "Embeddding 794 generated\n",
            "Embeddding 795 generated\n",
            "Embeddding 796 generated\n",
            "Embeddding 797 generated\n",
            "Embeddding 798 generated\n",
            "Embeddding 799 generated\n",
            "Embeddding 800 generated\n",
            "Embeddding 801 generated\n",
            "Embeddding 802 generated\n",
            "Embeddding 803 generated\n",
            "Embeddding 804 generated\n",
            "Embeddding 805 generated\n",
            "Embeddding 806 generated\n",
            "Encoding Successful\n",
            "No embeddings failed\n",
            "Written to csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using the Tool"
      ],
      "metadata": {
        "id": "SHk2atoI5GSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`make sure you run the *\"some functions to define\"* section even if you're not encoding any new context`"
      ],
      "metadata": {
        "id": "LAGt78jo-6Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After Once the corpus has been encoded once, the embeddings just needs to be loaded once each session."
      ],
      "metadata": {
        "id": "eBx9qN_wdl8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = load_embeddings(fpath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChWq3Y92--OE",
        "outputId": "7893bccc-087e-4bd6-fb8e-89cd7203a848"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have two lists, one with the corpus text and one with the encoded text (vector embedding), we'll aggregate them into one data structure."
      ],
      "metadata": {
        "id": "81C_ejbZdahi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_text_and_embeddings(text_to_encode, embeddings) -> Dict[str , Tuple[str, List[float]]]:\n",
        "  assert embeddings is not None\n",
        "\n",
        "  text_and_embeddings = {}\n",
        "  for idx, text in enumerate(text_to_encode):\n",
        "    text_and_embeddings[f\"Chunk {idx + 1}\"] = (text, embeddings[idx])\n",
        "  return text_and_embeddings\n",
        "\n",
        "# Example\n",
        "# {\"Chunk 1\" : (\"This is the text of chunk 1...\", [.23434, .12324, .52323, ...]),\n",
        "#  \"Chunk 2\" : (\"This is the text of chunk 2...\", [.20934, .16524, .78362, ...])}"
      ],
      "metadata": {
        "id": "F41Sjt35h_DS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the text and embeddings get aggregated into one dictionary called \"corpus\""
      ],
      "metadata": {
        "id": "0kOusLKleTXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_and_embeddings = aggregate_text_and_embeddings(dsm_list, embeddings)"
      ],
      "metadata": {
        "id": "CbEwHNb-zZ7w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to aggreate the text of each chunk to its respective vector embedding"
      ],
      "metadata": {
        "id": "CxE2OU9ho4Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions caluclate similarity between the question embedding and each corpus embedding using a dot product, then return the text chunks in order of similarity"
      ],
      "metadata": {
        "id": "pMc_d_q4o-PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_similarity(x: List[float], y: List[float]) -> float:\n",
        "    return np.dot(np.array(x), np.array(y))\n",
        "\n",
        "def order_by_similarity(question : str, text_and_embeddings : Dict[str, Tuple[str, List[float]]]) -> List[Tuple[float, str, str]]:\n",
        "  \n",
        "  # include the case note in the quesiton embedding\n",
        "  question_embedding = get_embedding(f\"{case_note}\\n{question}\")\n",
        "  # don't include the case note in the question embedding\n",
        "  question_embedding = get_embedding(question)\n",
        "\n",
        "  ordered_text = sorted([(vector_similarity(question_embedding, chunk[1]), chunk, idx) for idx, chunk in text_and_embeddings.items()], reverse=True)\n",
        "\n",
        "  print(\"Text ordered by similarity\")\n",
        "  return ordered_text\n",
        "\n",
        "# Example\n",
        "# [(0.12398640147, \"Chunk 47\", \"This is the text of chunk 47...\")...]"
      ],
      "metadata": {
        "id": "sX8Yn4uvdpJC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the prompt**"
      ],
      "metadata": {
        "id": "aAQRRowEpcAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This term is used to separate the sections of text that get pulled into the prompt each time it gets built. Checking the number of tokens so that we stay in adherance to all our token limits down the line."
      ],
      "metadata": {
        "id": "PTHN9R2yetXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seperator = \"\\n* \"\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "separator_len = len(tokenizer.tokenize(SEPARATOR))\n",
        "\n",
        "f\"Context separator contains {separator_len} tokens\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SOo6yDjK22RF",
        "outputId": "626cb504-5805-46f7-f512-4d009e9dbb49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Context separator contains 3 tokens'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This funciton builds the prompt, which is a string that is sent to OpenAI's GPT-3 for the final response\n",
        "\n",
        "If you want to modify the header, do it here"
      ],
      "metadata": {
        "id": "YLq-BTCHe7UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(question : str, text_and_embeddings : Dict[str , Tuple[str, List[float]]]) -> str:\n",
        "  # first we order the context by how simlar their embeddings are to our query embedding\n",
        "  ordered_text = order_by_similarity(question, text_and_embeddings)\n",
        "\n",
        "  context_list = []\n",
        "  context_tokens = 0\n",
        "  context_token_limit = 2000\n",
        "\n",
        "  # then we append the most similar context until we reach our token limit\n",
        "  for item in ordered_text:\n",
        "    text = item[1][0]\n",
        "\n",
        "    context_tokens += len(tokenizer.tokenize(text)) + separator_len\n",
        "\n",
        "    if context_tokens > context_token_limit:\n",
        "      break\n",
        "\n",
        "    context_list.append(seperator + text.replace(\"\\n\", \" \"))\n",
        "\n",
        "  # now to actually construct the prompt\n",
        "  header = \"\"\"Answer as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
        "\n",
        "  prompt = header + \"\".join(context_list) + \"\\n\\nCase Note: \\n\" + case_note + \"\\n\\nQuestion: \" + question + \"\\nAnswer:\"\n",
        "\n",
        "  print(\"Prompt Constructed: \\n\")\n",
        "  print(prompt)\n",
        "\n",
        "  return prompt\n"
      ],
      "metadata": {
        "id": "bb9L5lUjeGoa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can modify the input below"
      ],
      "metadata": {
        "id": "gCFBHSmg7xE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Screen this patient for depression.\n",
        "\n",
        "1. Identify the patient\n",
        "2. List the patient's core symptoms\n",
        "3. List other key patient information\n",
        "4. Figure out if the patient meets any diagnostic criteria\n",
        "5. Decide if the patient meets enough criteria to warrant further screening for anything\"\"\""
      ],
      "metadata": {
        "id": "OcKEAT7HnraH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case_note = \"\"\"\n",
        "Michael is a 15-year-old male who presents with fatigue and lack of interest in activities. He has lost weight and his parents are concerned. He denies any other symptoms. Medical history is significant for normal development. He has no known allergies and is up to date on his vaccinations. He reports that he used to be a good student but his grades have been slipping and he’s been skipping class. He used to be popular with his classmates but now he feels like he doesn’t fit in and has no friends. He denies any history of bullying. Mental status exam reveals a cooperative, well-groomed male who is alert and oriented to person, place, and time. He has a flat affect and speaks in a monotone voice. He reports feeling “down” but denies any suicidal ideation\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MKJk5dDgbO3Z"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get your response"
      ],
      "metadata": {
        "id": "bBWJhBPr70eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_response(question, text_and_embeddings)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG3nnQsOATYQ",
        "outputId": "6e419a7a-efe7-4895-c321-9af69399f11b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text ordered by similarity\n",
            "Prompt Constructed: \n",
            "\n",
            "Answer as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n",
            "\n",
            "Context:\n",
            "\n",
            "* Other Specified Depressive Disorder  183  condition, b) the probability that the associated medical condition has a potential to pro- mote or cause a depressive disorder, and c) a course of the depressive symptoms shortly after the onset or worsening of the medical condition, especially if the depressive symp- toms remit near the time that the medical disorder is effectively treated or remits.  Medication-induced depressive disorder. An important caveat is that some medical con- ditions are treated with medications (e.g., steroids or alpha-interferon) that can induce depres- sive or manic symptoms. In these cases, clinical judgment, based on all the evidence in hand, is the best way to try to separate the most likely and/or the most important of two etiological fac- tors (i.e., association with the medical condition vs. a substance-induced syndrome).  Adjustment disorders. It is important to differentiate a depressive episode from an ad- justment disorder, as the onset of the medical condition is in itself a life stressor that could bring on either an adjustment disorder or an episode of major depression. The major dif- ferentiating elements are the pervasiveness the depressive picture and the number and quality of the depressive symptoms that the patient reports or demonstrates on the mental status examination. The differential diagnosis of the associated medical conditions is rel- evant but largely beyond the scope of the present manual.   Comorbidity Conditions comorbid with depressive disorder due to another medical condition are those associated with the medical conditions of etiological relevance. It has been noted that de- lirium can occur before or along with depressive symptoms in individuals with a variety of medical conditions, such as Cushing’s disease. The association of anxiety symptoms, usually generalized symptoms, is common in depressive disorders, regardless of cause.  Other Specified Depressive Disorder  311 (F32.8)  This category applies to presentations in which symptoms characteristic of a depressive disorder that cause clinically significant distress or impairment in social, occupational, or other important areas of functioning predominate but do not meet the full criteria for any of the disorders in the depressive disorders diagnostic class. The other specified depressive disorder category is used in situations in which the clinician chooses to communicate the specific reason that the presentation does not meet the criteria for any specific depressive disorder. This is done by recording “other specified depressive disorder” followed by the specific reason (e.g., “short-duration depressive episode”).  Examples of presentations that can be specified using the “other specified” designation  include the following: 1. Recurrent brief depression: Concurrent presence of depressed mood and at least four other symptoms of depression for 2–13 days at least once per month (not associ- ated  with  the  menstrual  cycle)  for  at  least  12  consecutive  months  in  an  individual whose presentation has never met criteria for any other depressive or bipolar disorder and does not currently meet active or residual criteria for any psychotic disorder. 2. Short-duration depressive episode (4–13 days): Depressed affect and at least four of the other eight symptoms of a major depressive episode associated with clinically significant distress or impairment that persists for more than 4 days, but less than 14 days, in an individual whose presentation has never met criteria for any other depressive or bipolar disorder, does not currently meet active or residual criteria for any psychotic dis- order, and does not meet criteria for recurrent brief depression.  3. Depressive episode with insufficient symptoms: Depressed affect and at least one of the other eight symptoms of a major depressive episode associated with clinically  \f\n",
            "* 164  Depressive Disorders  substantial effort. The efficiency with which tasks are accomplished may be reduced. For example, an individual may complain that washing and dressing in the morning are ex- hausting and take twice as long as usual.  The sense of worthlessness or guilt associated with a major depressive episode may in- clude unrealistic negative evaluations of one’s worth or guilty preoccupations or rumina- tions over minor past failings (Criterion A7). Such individuals often misinterpret neutral or trivial day-to-day events as evidence of personal defects and have an exaggerated sense of responsibility for untoward events. The sense of worthlessness or guilt may be of delu- sional proportions (e.g., an individual who is convinced that he or she is personally re- sponsible for world poverty). Blaming oneself for being sick and for failing to meet occupational or interpersonal responsibilities as a result of the depression is very common and, unless delusional, is not considered sufficient to meet this criterion.  Many individuals report impaired ability to think, concentrate, or make even minor decisions (Criterion A8). They may appear easily distracted or complain of memory diffi- culties. Those engaged in cognitively demanding pursuits are often unable to function. In children, a precipitous drop in grades may reflect poor concentration. In elderly individ- uals, memory difficulties may be the chief complaint and may be mistaken for early signs of a dementia (“pseudodementia”). When the major depressive episode is successfully treated, the memory problems often fully abate. However, in some individuals, particu- larly elderly persons, a major depressive episode may sometimes be the initial presenta- tion of an irreversible dementia.  Thoughts of death, suicidal ideation, or suicide attempts (Criterion A9) are common. They may range from a passive wish not to awaken in the morning or a belief that others would be better off if the individual were dead, to transient but recurrent thoughts of com- mitting suicide, to a specific suicide plan. More severely suicidal individuals may have put their affairs in order (e.g., updated wills, settled debts), acquired needed materials (e.g., a rope or a gun), and chosen a location and time to accomplish the suicide. Motivations for suicide may include a desire to give up in the face of perceived insurmountable obstacles, an intense wish to end what is perceived as an unending and excruciatingly painful emo- tional state, an inability to foresee any enjoyment in life, or the wish to not be a burden to others. The resolution of such thinking may be a more meaningful measure of diminished suicide risk than denial of further plans for suicide.  The evaluation of the symptoms of a major depressive episode is especially difficult when they occur in an individual who also has a general medical condition (e.g., cancer, stroke, myocardial infarction, diabetes, pregnancy). Some of the criterion signs and symp- toms of a major depressive episode are identical to those of general medical conditions (e.g., weight loss with untreated diabetes; fatigue with cancer; hypersomnia early in preg- nancy; insomnia later in pregnancy or the postpartum). Such symptoms count toward a major depressive diagnosis except when they are clearly and fully attributable to a general medical condition. Nonvegetative symptoms of dysphoria, anhedonia, guilt or worthless- ness, impaired concentration or indecision, and suicidal thoughts should be assessed with particular care in such cases. Definitions of major depressive episodes that have been mod- ified to include only these nonvegetative symptoms appear to identify nearly the same in- dividuals as do the full criteria.  Associated Features Supporting Diagnosis  Major depressive disorder is associated with high mortality, much of which is accounted for by suicide; however, it is not the only cause. For example, depressed individuals ad- mitted to nursing homes have a markedly increased likelihood of death in the first year. In- dividuals frequently present with tearfulness, irritability, brooding, obsessive rumination, anxiety, phobias, excessive worry over physical health, and complaints of pain (e.g., head- aches; joint, abdominal, or other pains). In children, separation anxiety may occur.   \f\n",
            "\n",
            "Case Note: \n",
            "\n",
            "Michael is a 15-year-old male who presents with fatigue and lack of interest in activities. He has lost weight and his parents are concerned. He denies any other symptoms. Medical history is significant for normal development. He has no known allergies and is up to date on his vaccinations. He reports that he used to be a good student but his grades have been slipping and he’s been skipping class. He used to be popular with his classmates but now he feels like he doesn’t fit in and has no friends. He denies any history of bullying. Mental status exam reveals a cooperative, well-groomed male who is alert and oriented to person, place, and time. He has a flat affect and speaks in a monotone voice. He reports feeling “down” but denies any suicidal ideation\n",
            "\n",
            "\n",
            "Question: \n",
            "Screen this patient for depression.\n",
            "\n",
            "1. Identify the patient\n",
            "2. List the patient's core symptoms\n",
            "3. List other key patient information\n",
            "4. Figure out if the patient meets any diagnostic criteria\n",
            "5. Decide if the patient meets enough criteria to warrant further screening for anything\n",
            "Answer:\n",
            "1. Michael, a 15-year-old male\n",
            "2. Fatigue, lack of interest in activities, weight loss, slipping grades, social withdrawal, flat affect\n",
            "3. No known allergies or medical conditions. Up to date on vaccinations.\n",
            "4. The patient meets the criteria for a major depressive episode.\n",
            "5. Yes, the patient should be further screened for depression.\n"
          ]
        }
      ]
    }
  ]
}